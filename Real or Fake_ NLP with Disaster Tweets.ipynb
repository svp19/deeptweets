{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"from __future__ import absolute_import, division, print_function, unicode_literals\nimport tensorflow.compat.v1 as tf\ntf.disable_eager_execution()\nimport tensorflow_hub as hub\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport re\nimport seaborn as sns\nfrom keras import regularizers\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout, Concatenate\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\nfrom tensorflow.keras import backend as K\nnp.random.seed(10)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom absl import logging","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"\nembed = hub.KerasLayer(module_url)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=train[train['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=train[train['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\ntrain.keyword = train['keyword'].str.replace(\"[^a-zA-Z#]\", \" \")\nkeyword = train.keyword[train.keyword.notnull()].tolist()\nkeyword = Counter(keyword)\nkeywords = pd.DataFrame(keyword.most_common(10), columns=['Keyword', 'Count'])\nsns.set(rc={'figure.figsize':(14,6)})\nsns.barplot(data = keywords, x = 'Keyword', y = 'Count')\nplt.title(\"Most Common Keywords\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(train['target'])\nplt.title(\"Distribution Of Target\")\nsns.set(rc={'figure.figsize':(10,8)})\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"'''\nimport re\nimport string\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('[‘’“”…]', '', text)\n    text = re.sub('\\n', '', text)\n    return text\n\ntrain['text'] = train.text.apply(lambda x: clean_text(x))\ntrain['text'] = train['text'].str.replace(\"[^a-zA-Z#]\", \" \")\ntrain['text'] = train['text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\ndef remove_URL(text):\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ntrain['text']=train['text'].apply(lambda x : remove_URL(x))\ntest['text']=test['text'].apply(lambda x : remove_URL(x))\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n\ntrain['text']=train['text'].apply(lambda x : remove_html(x))\ntest['text']=test['text'].apply(lambda x : remove_html(x))\n\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake 😔😔\")\n\n\ntrain['text']=train['text'].apply(lambda x : remove_emoji(x))\ntest['text']=test['text'].apply(lambda x : remove_emoji(x))\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #king\"\nprint(remove_punct(example))\n\ntrain['text']=train['text'].apply(lambda x : remove_punct(x))\ntest['text']=test['text'].apply(lambda x : remove_punct(x))\n\n!pip install pyspellchecker\n\nfrom spellchecker import SpellChecker\n\n'''\npell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n        \ntext = \"corect me plese\"\ncorrect_spellings(text)\n\ntrain['text']=train['text'].apply(lambda x : correct_spellings(x))\ntest['text']=test['text'].apply(lambda x : correct_spellings(x))\n'''\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.keyword = train.keyword.fillna(\"\")\ntrain['new_text'] = train.text + \" \" +train.keyword\n\ntest.keyword = test.keyword.fillna(\"\")\ntest['text'] = test.text + \" \" +test.keyword\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train.new_text.values, train.target.values, test_size=0.2, random_state=42)\ntest_data = test.text.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def build_model(embed):\n    model = Sequential([\n        Input(shape=[], dtype=tf.string),\n        embed,\n        Dense(512,activation='relu'),\n        BatchNormalization(),\n        Dropout(0.1),\n        Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.02)),\n        BatchNormalization(),\n        Dropout(0.05),\n        Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.02)),\n        BatchNormalization(),\n        Dropout(0.05),\n        Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.02)),\n        BatchNormalization(),\n        Dense(1, activation='sigmoid')\n    ])\n    model.compile(Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n    '''\n        Dense(1024, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.1),\n    '''\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow_hub as hub\nimport tensorflow as tf\n#module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"\nmodule_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]\n\nembed_new = hub.Module(module_url)\nwith tf.compat.v1.Session() as session:\n  session.run([tf.compat.v1.global_variables_initializer(), tf.compat.v1.tables_initializer()])\n  sentences_embeddings = session.run(embed_new(X_train))\n  sentences_embeddings_test = session.run(embed_new(X_test))\n  test_embedding = session.run(embed_new(test_data))\n\ntrain_embeddings = np.array(sentences_embeddings)\ntest_embeddings = np.array(sentences_embeddings_test)\ntest_data_embeddings = np.array(test_embedding)\n  \n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nclf = SVC(gamma='auto')\nclf.fit(train_embeddings, y_train)\nclf.score(test_embeddings,y_test)\nsub = clf.predict(test_data_embeddings)\nsubm = pd.DataFrame()\nsubm['id'] = test['id']\nsubm['target'] = sub.round().astype(int)\nsubm.to_csv(\"pred_svm.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import LSTM\nmodel = build_model(embed)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\nmcp_save = ModelCheckpoint('model.h5', save_best_only=True, monitor='val_loss', mode='min')\nreduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_train[0])\nprint(y_train.shape)\nwith tf.compat.v1.Session() as session:\n    tf.compat.v1.keras.backend.set_session(session)\n    session.run([tf.compat.v1.global_variables_initializer(), tf.compat.v1.tables_initializer()])\n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_test,y_test),\n        epochs=35,\n        callbacks=[earlyStopping,mcp_save,reduce_lr_loss],\n        batch_size=16\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.subplot(2,2,1)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\n#plt.show()\nplt.subplot(2,2,2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"with tf.Session() as session:\n    tf.compat.v1.keras.backend.set_session(session)\n    session.run(tf.global_variables_initializer())\n    session.run(tf.tables_initializer())\n    model.load_weights('model_new.h5')\n    y_pred = model.predict(X_test)\n    \nfrom sklearn.metrics import confusion_matrix, classification_report\n#print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred.round().astype(int)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tf.Session() as session:\n    tf.compat.v1.keras.backend.set_session(session)\n    session.run(tf.global_variables_initializer())\n    session.run(tf.tables_initializer())\n    model.load_weights('model_new.h5')\n    sub = model.predict(test_data)\n    \n\nsubm = pd.DataFrame()\nsubm['id'] = test['id']\nsubm['target'] = sub.round().astype(int)\nsubm.to_csv(\"pred_new.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}